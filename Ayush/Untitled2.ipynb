{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, data_dir='./', hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.data_dir = data_dir\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "        # Define PyTorch model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels * width * height, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init our model\n",
    "mnist_model = LitMNIST()\n",
    "\n",
    "# Init DataLoader from MNIST Dataset\n",
    "# train_ds = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n",
    "# train_loader = DataLoader(train_ds, batch_size=32)\n",
    "\n",
    "# # Initialize a trainer\n",
    "# trainer = pl.Trainer(gpus=None, max_epochs=3, progress_bar_refresh_rate=20)\n",
    "\n",
    "# # Train the model ⚡\n",
    "# trainer.fit(mnist_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHECKPOINT_HYPER_PARAMS_KEY',\n",
      " 'CHECKPOINT_HYPER_PARAMS_NAME',\n",
      " 'CHECKPOINT_HYPER_PARAMS_TYPE',\n",
      " '_DeviceDtypeModuleMixin__update_properties',\n",
      " '_LightningModule__auto_choose_log_on_epoch',\n",
      " '_LightningModule__auto_choose_log_on_step',\n",
      " '_LightningModule__get_hparams_assignment_variable',\n",
      " '__abstractmethods__',\n",
      " '__call__',\n",
      " '__class__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__eq__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattr__',\n",
      " '__getattribute__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__jit_unused_properties__',\n",
      " '__le__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__setstate__',\n",
      " '__sizeof__',\n",
      " '__slots__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_abc_impl',\n",
      " '_apply',\n",
      " '_auto_collect_arguments',\n",
      " '_backward_hooks',\n",
      " '_buffers',\n",
      " '_current_dataloader_idx',\n",
      " '_current_fx_name',\n",
      " '_current_hook_fx_name',\n",
      " '_datamodule',\n",
      " '_device',\n",
      " '_dtype',\n",
      " '_example_input_array',\n",
      " '_forward_hooks',\n",
      " '_forward_pre_hooks',\n",
      " '_get_name',\n",
      " '_load_from_state_dict',\n",
      " '_load_model_state',\n",
      " '_load_state_dict_pre_hooks',\n",
      " '_modules',\n",
      " '_named_members',\n",
      " '_parameters',\n",
      " '_register_load_state_dict_pre_hook',\n",
      " '_register_state_dict_hook',\n",
      " '_replicate_for_data_parallel',\n",
      " '_results',\n",
      " '_running_manual_backward',\n",
      " '_save_to_state_dict',\n",
      " '_set_hparams',\n",
      " '_slow_forward',\n",
      " '_state_dict_hooks',\n",
      " '_verify_is_manual_optimization',\n",
      " '_version',\n",
      " 'add_module',\n",
      " 'all_gather',\n",
      " 'apply',\n",
      " 'automatic_optimization',\n",
      " 'backward',\n",
      " 'bfloat16',\n",
      " 'buffers',\n",
      " 'children',\n",
      " 'configure_optimizers',\n",
      " 'cpu',\n",
      " 'cuda',\n",
      " 'current_epoch',\n",
      " 'data_dir',\n",
      " 'datamodule',\n",
      " 'device',\n",
      " 'dims',\n",
      " 'double',\n",
      " 'dtype',\n",
      " 'dump_patches',\n",
      " 'eval',\n",
      " 'example_input_array',\n",
      " 'exp_save_path',\n",
      " 'extra_repr',\n",
      " 'float',\n",
      " 'forward',\n",
      " 'freeze',\n",
      " 'get_progress_bar_dict',\n",
      " 'global_step',\n",
      " 'grad_norm',\n",
      " 'half',\n",
      " 'hidden_size',\n",
      " 'hparams',\n",
      " 'hparams_initial',\n",
      " 'learning_rate',\n",
      " 'load_from_checkpoint',\n",
      " 'load_state_dict',\n",
      " 'loaded_optimizer_states_dict',\n",
      " 'log',\n",
      " 'log_dict',\n",
      " 'logger',\n",
      " 'manual_backward',\n",
      " 'model',\n",
      " 'modules',\n",
      " 'named_buffers',\n",
      " 'named_children',\n",
      " 'named_modules',\n",
      " 'named_parameters',\n",
      " 'num_classes',\n",
      " 'on_after_backward',\n",
      " 'on_before_zero_grad',\n",
      " 'on_epoch_end',\n",
      " 'on_epoch_start',\n",
      " 'on_fit_end',\n",
      " 'on_fit_start',\n",
      " 'on_gpu',\n",
      " 'on_hpc_load',\n",
      " 'on_hpc_save',\n",
      " 'on_load_checkpoint',\n",
      " 'on_pretrain_routine_end',\n",
      " 'on_pretrain_routine_start',\n",
      " 'on_save_checkpoint',\n",
      " 'on_test_batch_end',\n",
      " 'on_test_batch_start',\n",
      " 'on_test_epoch_end',\n",
      " 'on_test_epoch_start',\n",
      " 'on_test_model_eval',\n",
      " 'on_test_model_train',\n",
      " 'on_train_batch_end',\n",
      " 'on_train_batch_start',\n",
      " 'on_train_end',\n",
      " 'on_train_epoch_end',\n",
      " 'on_train_epoch_start',\n",
      " 'on_train_start',\n",
      " 'on_validation_batch_end',\n",
      " 'on_validation_batch_start',\n",
      " 'on_validation_epoch_end',\n",
      " 'on_validation_epoch_start',\n",
      " 'on_validation_model_eval',\n",
      " 'on_validation_model_train',\n",
      " 'optimizer_step',\n",
      " 'optimizer_zero_grad',\n",
      " 'optimizers',\n",
      " 'parameters',\n",
      " 'precision',\n",
      " 'prepare_data',\n",
      " 'print',\n",
      " 'register_backward_hook',\n",
      " 'register_buffer',\n",
      " 'register_forward_hook',\n",
      " 'register_forward_pre_hook',\n",
      " 'register_parameter',\n",
      " 'requires_grad_',\n",
      " 'save_hyperparameters',\n",
      " 'setup',\n",
      " 'share_memory',\n",
      " 'state_dict',\n",
      " 'summarize',\n",
      " 'tbptt_split_batch',\n",
      " 'teardown',\n",
      " 'test_dataloader',\n",
      " 'test_epoch_end',\n",
      " 'test_step',\n",
      " 'test_step_end',\n",
      " 'to',\n",
      " 'to_onnx',\n",
      " 'to_torchscript',\n",
      " 'toggle_optimizer',\n",
      " 'train',\n",
      " 'train_dataloader',\n",
      " 'trainer',\n",
      " 'training',\n",
      " 'training_epoch_end',\n",
      " 'training_step',\n",
      " 'training_step_end',\n",
      " 'transfer_batch_to_device',\n",
      " 'transform',\n",
      " 'type',\n",
      " 'unfreeze',\n",
      " 'use_amp',\n",
      " 'use_ddp',\n",
      " 'use_ddp2',\n",
      " 'use_dp',\n",
      " 'use_tpu',\n",
      " 'val_dataloader',\n",
      " 'validation_epoch_end',\n",
      " 'validation_step',\n",
      " 'validation_step_end',\n",
      " 'write_prediction',\n",
      " 'write_prediction_dict',\n",
      " 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dir(mnist_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 55.1 K\n",
      "-------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "/Users/ayushd/opt/anaconda3/envs/pytl/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cc47c6c3844cbbb7e9c3b55f915259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushd/opt/anaconda3/envs/pytl/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17353cfdcd974cc8aedceae07457de60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ec024d10ea4f2dabbe0ef8b936816a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283ab18ce41e4085bbfa43a1f990110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushd/opt/anaconda3/envs/pytl/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=None, max_epochs=3, progress_bar_refresh_rate=20)\n",
    "trainer.fit(mnist_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
