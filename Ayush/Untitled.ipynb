{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import scipy.misc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\\\n",
    "import pickle as pkl\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "data_file = \"shuffled_labels_bin_17.csv\"\n",
    "batch_size = 8\n",
    "lr = 0.001\n",
    "num_epochs = 5\n",
    "num_output = 1\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "cur_transform_tr = transforms.Compose([\n",
    "transforms.Resize(256),\n",
    "transforms.CenterCrop(224),\n",
    "transforms.RandomHorizontalFlip(),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(\n",
    "mean=[0.485, 0.456, 0.406],\n",
    "std=[0.229, 0.224, 0.225]\n",
    ")])\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "cur_transform_val = transforms.Compose([\n",
    "transforms.Resize(256),\n",
    "transforms.CenterCrop(224),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(\n",
    "mean=[0.485, 0.456, 0.406],\n",
    "std=[0.229, 0.224, 0.225]\n",
    ")])\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "data_transforms = {}\n",
    "data_transforms['train'] = cur_transform_tr\n",
    "data_transforms['val'] = cur_transform_val\n",
    "data_transforms['test'] = cur_transform_val\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "image_root = \"Interior_Images/\"\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def load_next_image(image_path):\n",
    "    image_path=os.path.join(image_root,image_path)\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "class Nerlp_Images(Dataset):\n",
    "    def __init__(self, filename,transform, mode='train'):\n",
    "        self.transform = transform\n",
    "        nerlp = pd.read_csv(filename, index_col=None)\n",
    "#         self.features = ['Mattress','Pressure_cooker','Chair','Bed','Table','Clock','Electric_fan','Radio','Sewing_machine','Mobile','Landline','Refrigerator','Air_con_or_Cooler','Washing_Machine','Internet','Computer','Television']\n",
    "#         self.features = ['Wealth_Index']\n",
    "#         nerlp=nerlp.sample(frac=1)\n",
    "        self.features = [sys.argv[1]]\n",
    "        self.nerlp = nerlp[self.features].values\n",
    "#         mx_val=np.amax(self.nerlp,axis=0)\n",
    "#         self.nerlp = np.divide(self.nerlp,mx_val)\n",
    "        self.houseids = nerlp['uniq_id'].values\n",
    "        length=len(self.houseids)\n",
    "        if(mode=='train'):\n",
    "            start=int(0*length)\n",
    "            end=int(0.8*length)\n",
    "        elif(mode=='val'):\n",
    "            start=int(0.8*length)\n",
    "            end=int(0.9*length)\n",
    "        else:\n",
    "            start=int(0.9*length)\n",
    "            end=int(1*length)\n",
    "        \n",
    "        self.nerlp=self.nerlp[start:end]\n",
    "        self.houseids=self.houseids[start:end]\n",
    "            \n",
    "        # print(type(self.houseids))\n",
    "        # print(self.houseids.shape)\n",
    "        # self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nerlp.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        y = self.nerlp[idx].astype('float64')\n",
    "        house_code = self.houseids[idx]\n",
    "        image_name=str(house_code)+\"_HouseInterior_6.jpg\"\n",
    "        x = load_next_image(image_name)\n",
    "        x = self.transform(x)\n",
    "        # print(type(x))\n",
    "        # print(y.shape)\n",
    "        if(x.shape[0]>3):\n",
    "            print(\"********\",image_name,\"**********\",x.shape)\n",
    "        x=torch.tensor(x)\n",
    "        y=torch.tensor(y)\n",
    "        # print(x.shape)\n",
    "        # print(y.shape)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "dsets = {\n",
    "    x: Nerlp_Images(filename=data_file, transform=data_transforms[x], mode=x)\n",
    "    for x in ['train', 'val','test']\n",
    "}\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "# dtype=torch.FloatTensor\n",
    "dtype = torch.LongTensor\n",
    "dset_loaders = {\n",
    "    x: torch.utils.data.DataLoader(dsets[x], batch_size=batch_size, shuffle=(x == 'train'),num_workers=4)\n",
    "    for x in ['train', 'val','test']\n",
    "}\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def get_R2(y, y_):\n",
    "#     y = y.data.cpu().numpy()\n",
    "#     y_ = y_.data.cpu().numpy()\n",
    "    y_mean = np.mean(y, axis=0)\n",
    "    ss_pred = np.linalg.norm(y-y_, axis=0)\n",
    "    ss_reg = np.linalg.norm(y-y_mean, axis=0)\n",
    "    r_2 = ss_pred/ss_reg\n",
    "    return 1-r_2\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "features = [sys.argv[1]]\n",
    "# features = ['Mattress','Pressure_cooker','Chair','Bed','Table','Clock','Electric_fan','Radio','Sewing_machine','Mobile','Landline','Refrigerator','Air_con_or_Cooler','Washing_Machine','Internet','Computer','Television']\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = -1.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            run_y = np.zeros((0,),dtype=np.int32)\n",
    "            run_y_ = np.zeros((0,),dtype=np.int32)\n",
    "            # Iterate over data.\n",
    "            cnt = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                cnt = cnt + 1\n",
    "                labels = labels.type(dtype)\n",
    "                labels = torch.squeeze(labels,dim=1)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "#                         print(outputs.data.cpu().numpy().shape)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                run_y = np.concatenate((run_y,labels.data.cpu().numpy()),axis=0)\n",
    "                run_y_ = np.concatenate((run_y_,preds.data.cpu().numpy()),axis=0)\n",
    "                \n",
    "#                 run_epoch_loss = running_loss / (cnt*batch_size)\n",
    "#                 run_epoch_R2 = get_R2(run_y,run_y_)\n",
    "\n",
    "#                 print('{} rLoss: {:.4f} rR2_AHI: {:.4f} rR2_WI: {:.4f}'.format(phase, run_epoch_loss, run_epoch_R2[0],run_epoch_R2[1]))\n",
    "        \n",
    "        \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#             epoch_R2 = get_R2(run_y,run_y_)\n",
    "#             epoch_acc = np.mean(epoch_R2)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "#             print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "#             for i in range(len(features)):\n",
    "#                 print('R2_'+features[i]+' : ',epoch_R2[i])\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print(confusion_matrix(run_y,run_y_))\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# output 2 because 2 classes\n",
    "model_ft.fc = nn.Linear(num_ftrs,2)\n",
    "input_size = 224\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "print(model_ft)\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "params_to_update = []\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(params=params_to_update, lr=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "model_ft, hist = train_model(model_ft,dset_loaders, criterion, optimizer_ft, num_epochs=num_epochs)\n",
    "\n",
    "pkl_dump = open(sys.argv[1]+'_cross_entropy.pkl','wb')\n",
    "pkl.dump(model_ft,pkl_dump)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
